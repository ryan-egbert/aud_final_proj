# -*- coding: utf-8 -*-
"""models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oTE-0pSTVbfkGg41pQh1PYFJPvF-UHVK

## Imports
"""

print("--- Importing Packages ---")
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.svm import LinearSVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import jaccard_score
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

"""## Data Preprocessing

### Load/Process Data
"""

print("--- Load/Process Data ---")
df = pd.read_csv("../csv/data.csv", header=None)
df.columns = ('category', 'text', 'postid')  # rename columns

# Convert Categories to numerical values
id = 1
seen = {}  # categories that have been encountered
for idx, row in df.iterrows():
    # Resumes
    if row.category == 'resumes / job wanted':
        df.loc[idx, 'category'] = 0
    # Jobs
    else:
        if row.category in seen:
            df.loc[idx, 'category'] = seen[row.category]
        else:
            df.loc[idx, 'category'] = id
            seen[row.category] = id
            id += 1

job_list = df[df["category"] != 0].copy()
resume_list = df[df["category"] == 0].copy()

"""### Tokenization"""

print("--- Tokenization ---")
processed_collection_r = []
processed_collection_j = []
lemmatizer = nltk.stem.WordNetLemmatizer()

print("Resumes...")
for post in resume_list.text:
    tokens = nltk.word_tokenize(post)  # tokenize
    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]  # lemmatize
    tokens = [token for token in tokens if not token in stopwords.words('english') if token.isalpha()]  # remove stopwords
    joins = " ".join(tokens)  # reinsert updated review
    processed_collection_r.append(joins)

print("Jobs...")
for post in job_list.text:
    tokens = nltk.word_tokenize(post)  # tokenize
    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]  # lemmatize 
    tokens = [token for token in tokens if not token in stopwords.words('english') if token.isalpha()]  # remove stopwords
    joins = " ".join(tokens)  # reinsert updated review
    processed_collection_j.append(joins)

# Set up X and y sets
X = processed_collection_j
y = list(job_list.category)

"""### Split Train/Test Data"""

print("--- Train/Test Data ---")
#Split the data into training and test
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)

"""### Vectorization"""

print("--- Vectorization ---")
#Vectorize train and test data
vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1)

vectorizer.fit(x_train)
x_train_m = vectorizer.transform(x_train)
x_test_m = vectorizer.transform(x_test)

"""## Building the Models

### Naive Bayes
"""

print("< Naive Bayes >")
nb = MultinomialNB()
nb.fit(x_train_m, y_train)
y_pred_nb = nb.predict(x_test_m)

"""### SVM"""

print("< SVM >")
svm = LinearSVC(max_iter=2000, random_state=47906)
svm.fit(x_train_m, y_train)
y_pred_svm = svm.predict(x_test_m)

"""### Decision Tree"""

print("< Decision Tree >")
dt = DecisionTreeClassifier(max_depth=50, random_state=47906)
dt.fit(x_train_m, y_train)
y_pred_dt = dt.predict(x_test_m)

"""### Random Forest"""

print("< Random Forest >")
rf = RandomForestClassifier(n_estimators=100, max_depth=90, random_state=47906)
rf.fit(x_train_m, y_train)
y_pred_rf = rf.predict(x_test_m)

"""### Neural Network"""

print("< Neural Network >")
nn = MLPClassifier(solver='sgd', hidden_layer_sizes=(5,25,20), max_iter=1000, random_state=47906)
nn.fit(x_train_m, y_train)
y_pred_nn = nn.predict(x_test_m)

"""## Evaluating the Models

### Naive Bayes
"""

acc_nb = accuracy_score(y_test, y_pred_nb)
print("NB Accuracy: {}%".format(round(acc_nb*100,2)))

"""### SVM"""

acc_svm = accuracy_score(y_test, y_pred_svm)
print("SVM Accuracy: {}%".format(round(acc_svm*100, 2)))

"""### Decision Tree"""

acc_dt = accuracy_score(y_test, y_pred_dt)
print("Decision Tree Accuracy: {}%".format(round(acc_dt*100, 2)))

"""### Random Forest"""

acc_rf = accuracy_score(y_test, y_pred_rf)
print("RF Accuracy: {}%".format(round(acc_rf*100, 2)))

"""### Neural Network"""

acc_nn = accuracy_score(y_test, y_pred_nn)
print("NN Accuracy: {}%".format(round(acc_nn*100, 2)))

"""## Calculating Predicted Categories for Resumes"""

print("--- Calculating Predictions ---")
# Use SVM for prediction (highest performing model)
resume_vec = vectorizer.transform(processed_collection_r)
y_resume = svm.predict(resume_vec)

# Inverse of seen
categories = {v: k for k, v in seen.items()}

# Decode predictions and write into csv file
print("Writing predictions to 'predicted_categories.csv'...")
resume_list.loc[:,'pred_category'] = y_resume
resume_list.to_csv("../csv/predicted_categories.csv")

"""### Dividing by Category"""

# Revectorizing resumes and inserting into dictionary
print("--- Divide by Category ---")
res_idx_v = {}
for idx, row in resume_list.iterrows():
    resume_list.loc[idx, 'pred_category_text'] = categories[row.pred_category]
    post = row.text
    id = row.postid
    tokens = nltk.word_tokenize(post)
    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]
    tokens = [token for token in tokens if not token in stopwords.words('english') if token.isalpha()]
    joins = " ".join(tokens)
    v = vectorizer.transform([joins])
    if row.pred_category in res_idx_v:
        res_idx_v[row.pred_category].append((v, id))
    else:
        res_idx_v[row.pred_category] = [(v, id)]

# Revectorizing jobs and inserting into dictionary
jobs_idx_v = {}
for idx, row in job_list.iterrows():
    post = row.text
    id = row.postid
    tokens = nltk.word_tokenize(post)
    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]
    tokens = [token for token in tokens if not token in stopwords.words('english') if token.isalpha()]
    joins = " ".join(tokens)
    v = vectorizer.transform([joins])
    if row.category in jobs_idx_v:
        jobs_idx_v[row.category].append((v, id))
    else:
        jobs_idx_v[row.category] = [(v, id)]

"""## Find Top Resumes by Job"""

print("--- Top Resumes by Job ---")
#Predict top resumes that match each job using document comparasion
pred_resumes = pd.read_csv("../csv/predicted_categories.csv")

jobs_matching_resumes = {}
for job_category in jobs_idx_v:
    jobs = jobs_idx_v[job_category][:3]
    for job_vtext, job_id in jobs:
        v = job_vtext.toarray()
        if job_category in res_idx_v:
            v_text = res_idx_v[job_category]
            top = None
            sims = []
            for text, id in v_text:
                sim = cosine_similarity([v[0, :]], [text.toarray()[0, :]])
                sims.append((sim, id))

            sims.sort(key=lambda x: x[0], reverse=True)
            jobs_matching_resumes[job_id] = sims[:3]

"""### Save results"""

#Save results into csv file
print("Writing similarities to 'jobs_top_resumes.csv'...")
f = open('../csv/jobs_top_resumes.csv', 'w')
f.write('job_id,job_text,res1_id,res1_text,res2_id,res2_text,res3_id,res3_text\n')
for job_id in jobs_matching_resumes:
    job_text = list(df[df['postid'] == job_id].text)[0].replace(',','')
    f.write("{},{}".format(job_id, job_text))
    for resume in jobs_matching_resumes[job_id]:
        # Threshold for similarity score, reduce 
        if resume[0] > 0.04:
            resume_id = resume[1]
            resume_text = list(df[df['postid'] == resume_id].text)[0].replace(',','')
            f.write(",{},{}".format(resume_id,resume_text))
    f.write('\n')
f.close()

"""## Find Top Jobs by Resume"""

print("--- Top Jobs by Resume ---")
#Predict top jobs that matche each resume using document comparasion
resumes_matching_jobs = {}
for category in res_idx_v:
    potential_list = jobs_idx_v[category]
    for resume_vtext, resume_id in res_idx_v[category]:
        resume_vtext = resume_vtext.toarray()
        sims = []
        for job_vtext, job_id in potential_list:
            job_vtext = job_vtext.toarray()
            sim = cosine_similarity([resume_vtext[0, :]], [job_vtext[0, :]])
            sims.append((sim[0][0], job_id))

        sims.sort(key=lambda x: x[0], reverse=True)
        resumes_matching_jobs[resume_id] = sims[:3]

"""### Save results"""

#Save results into csv file
print("Writing similarities to 'resume_top_jobs.csv'...")
f = open('../csv/resume_top_jobs.csv', 'w')
f.write('res_id,res_text,job1_id,job1_text,job2_id,job2_text,job3_id,job3_text\n')
for res_id in resumes_matching_jobs:
    res_text = list(df[df['postid'] == res_id].text)[0].replace(',','')
    f.write("{},{}".format(res_id, res_text))
    for job in resumes_matching_jobs[res_id]:
        if job[0] > 0.04:
            job_id = job[1]
            job_text = list(df[df['postid'] == job_id].text)[0].replace(',','')
            f.write(",{},{}".format(job_id,job_text))
    f.write('\n')
f.close()