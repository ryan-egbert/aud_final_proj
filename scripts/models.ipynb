{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"../csv/data.csv\", header=None)\n",
    "df.columns = ('category', 'text', 'postid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>postid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>general labor</td>\n",
       "      <td>FERRARA CANDY *** PACKAGING ASSISTANTS *** Dek...</td>\n",
       "      <td>7405032484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer service</td>\n",
       "      <td>Office Customer Service Part - Time 24 hrs/wk ...</td>\n",
       "      <td>7405099747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resumes / job wanted</td>\n",
       "      <td>quality tech control (west burbs) Manufacturin...</td>\n",
       "      <td>7396174532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resumes / job wanted</td>\n",
       "      <td>Driver Job Wanted (Chicago) CDL-Class-B Lookin...</td>\n",
       "      <td>7400075704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transportation</td>\n",
       "      <td>$400/DAY Class A LOCAL/REGIONAL CDL Driver (Le...</td>\n",
       "      <td>7406024714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               category                                               text  \\\n",
       "0         general labor  FERRARA CANDY *** PACKAGING ASSISTANTS *** Dek...   \n",
       "1      customer service  Office Customer Service Part - Time 24 hrs/wk ...   \n",
       "2  resumes / job wanted  quality tech control (west burbs) Manufacturin...   \n",
       "3  resumes / job wanted  Driver Job Wanted (Chicago) CDL-Class-B Lookin...   \n",
       "4        transportation  $400/DAY Class A LOCAL/REGIONAL CDL Driver (Le...   \n",
       "\n",
       "       postid  \n",
       "0  7405032484  \n",
       "1  7405099747  \n",
       "2  7396174532  \n",
       "3  7400075704  \n",
       "4  7406024714  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 1\n",
    "seen = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if row.category == 'resumes / job wanted':\n",
    "        df.at[idx, 'category'] = 0\n",
    "    else:\n",
    "        if row.category in seen:\n",
    "            df.at[idx, 'category'] = seen[row.category]\n",
    "        else:\n",
    "            df.at[idx, 'category'] = id\n",
    "            seen[row.category] = id\n",
    "            id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'general labor': 1,\n",
       " 'customer service': 2,\n",
       " 'transportation': 3,\n",
       " 'manufacturing': 4,\n",
       " 'food/beverage/hospitality': 5,\n",
       " 'skilled trades/artisan': 6,\n",
       " 'et cetera': 7,\n",
       " 'nonprofit': 8,\n",
       " 'healthcare': 9,\n",
       " 'admin/office': 10,\n",
       " 'architect/engineer/cad': 11,\n",
       " 'sales': 12,\n",
       " 'technical support': 13,\n",
       " 'legal/paralegal': 14,\n",
       " 'real estate': 15,\n",
       " 'accounting/finance': 16,\n",
       " 'business/mgmt': 17,\n",
       " 'education/teaching': 18,\n",
       " 'retail/wholesale': 19,\n",
       " 'human resource': 20,\n",
       " 'software/qa/dba/etc': 21,\n",
       " 'salon/spa/fitness': 22,\n",
       " 'security': 23,\n",
       " 'art/media/design': 24,\n",
       " 'marketing/advertising/pr': 25,\n",
       " 'web/html/info design': 26,\n",
       " 'systems/networking': 27,\n",
       " 'writing/editing': 28}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list = df[df[\"category\"] != 0]\n",
    "resume_list = df[df[\"category\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_collection_r = []\n",
    "processed_collection_j = []\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for post in resume_list.text:\n",
    "    tokens = nltk.word_tokenize(post)\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "    tokens = [token for token in tokens if not token in stopwords.words('english') if token.isalpha()]\n",
    "    joins = \" \".join(tokens)\n",
    "    processed_collection_r.append(joins)\n",
    "\n",
    "for post in job_list.text:\n",
    "    tokens = nltk.word_tokenize(post)\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "    tokens = [token for token in tokens if not token in stopwords.words('english') if token.isalpha()]\n",
    "    joins = \" \".join(tokens)\n",
    "    processed_collection_j.append(joins)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_collection_j\n",
    "y = list(job_list.category)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=1)\n",
    "# vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=1)\n",
    "vectorizer.fit(x_train)\n",
    "x_train_m = vectorizer.transform(x_train)\n",
    "x_test_m = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.81%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(x_train_m, y_train)\n",
    "y_pred_nb = nb.predict(x_test_m)\n",
    "acc_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(\"Accuracy: {}%\".format(round(acc_nb*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 78.93%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# svm = make_pipeline(StandardScaler(with_mean=False), LinearSVC(random_state=123))\n",
    "svm = LinearSVC(random_state=123, max_iter=2000)\n",
    "\n",
    "svm.fit(x_train_m, y_train)\n",
    "y_pred_svm = svm.predict(x_test_m)\n",
    "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM Accuracy: {}%\".format(round(acc_svm*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.66      0.68       148\n",
      "           2       0.21      0.18      0.19        17\n",
      "           3       0.96      0.95      0.95       327\n",
      "           4       0.69      0.48      0.56        23\n",
      "           5       0.80      0.87      0.83       134\n",
      "           6       0.56      0.76      0.65        46\n",
      "           7       0.55      0.84      0.67        32\n",
      "           8       1.00      0.50      0.67         2\n",
      "           9       0.81      0.72      0.76        18\n",
      "          10       0.59      0.67      0.62        15\n",
      "          11       0.50      1.00      0.67         1\n",
      "          12       0.65      0.65      0.65        20\n",
      "          13       0.00      0.00      0.00         2\n",
      "          14       0.83      0.71      0.77         7\n",
      "          15       0.60      0.60      0.60         5\n",
      "          16       1.00      0.67      0.80         6\n",
      "          17       0.00      0.00      0.00         4\n",
      "          18       0.75      0.60      0.67         5\n",
      "          19       0.50      0.29      0.36         7\n",
      "          20       1.00      0.50      0.67         2\n",
      "          21       0.67      0.67      0.67         3\n",
      "          22       0.67      0.29      0.40         7\n",
      "          23       1.00      0.25      0.40         4\n",
      "          24       0.00      0.00      0.00         1\n",
      "          25       0.00      0.00      0.00         2\n",
      "          26       0.50      1.00      0.67         1\n",
      "          28       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.79       840\n",
      "   macro avg       0.58      0.51      0.52       840\n",
      "weighted avg       0.79      0.79      0.78       840\n",
      "\n",
      "[array([False, False])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/svm/_classes.py\", line 229, in fit\n",
      "    accept_large_sparse=False)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 802, in check_X_y\n",
      "    estimator=estimator)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: 'ccl label seeking manufacturing chemical engineer apply position sioux fall south dakota requires relocation south dakota relocation assistance available experience manufacturing engineer ccl label sioux fall sd looking manufacturing chemical engineer train next level manager exploding business high demand technically skilled employee understand need superior customer service join team enjoy working global organization specializing shrink sleeve pressure sensitive label south dakota personnel income state competitive pay commensurate experience skill level short term long term disability insurance paid company ccl label company industry exploding demand packaging facility equipped equipment looking individual take career next level organization reward employee excellent benefit packaging including medical dental vision std ltd life insurance gain share program gift card annual physicals gift card work anniversary tuition reimbursement scholarship program dependent child employee flexible work schedule education requirement ideal candidate degree mechanical chemical engineering qualification candidate must able perform production environment excellent interpersonal skill position requires lifting standing production environment long period time responsibility department supervision department scheduling prepare purchase requisition workflow organization definition data entry required report production prepare corrective action response audit response ensure action indeed preventing repeat occurrence small maintenance task requesting maintenance department assistance needed aspect employee management including interview train discipline annual review approval safety enforcement develop document best practice ensure first time right department output conduct document department training responsible machine uptime equipment identify act continuous improvement opportunity prepare participate facility department audit position supervise personnel activity equipment assigned production department duty assigned eoe employer apply'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/svm/_classes.py\", line 229, in fit\n",
      "    accept_large_sparse=False)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 802, in check_X_y\n",
      "    estimator=estimator)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: 'ferrara candy packaging assistant dekalb dekalb ferrara candy packaging assistant dekalb hiring packaging assistant new facility shift dekalb area great work environment great fun atmosphere free cell phone week job call text today apply e gurler road dekalb il surestaff leading provider light industrial logistics general labor production manufacturing labor southwest local convenient office surestaff equal opportunity employer eoe follow cdc illinois dept health guideline safety health call text today'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "scores = [~np.isnan(cross_val_score(svm, X, y, cv=2))]\n",
    "print(\"CV Accuracy: {}%\".format(round(scores[~np.isnan(scores)].mean()*100, 2)))\n",
    "\n",
    "# cm = confusion_matrix(y_test, y_pred_svm)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)#, display_labels=svm.classes_)\n",
    "# plt.rcParams['figure.figsize']=(400,400)\n",
    "# disp.plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy: 70.71%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=75, bootstrap=True, random_state=123)\n",
    "\n",
    "rf.fit(x_train_m, y_train)\n",
    "y_pred_rf = rf.predict(x_test_m)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"RF Accuracy: {}%\".format(round(acc_rf*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Accuracy: 43.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(solver='sgd', hidden_layer_sizes=(50), random_state=123, max_iter=500)\n",
    "nn.fit(x_train_m, y_train)\n",
    "y_pred_nn = nn.predict(x_test_m)\n",
    "acc_nn = accuracy_score(y_test, y_pred_nn)\n",
    "print(\"NN Accuracy: {}%\".format(round(acc_nn*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_vec = vectorizer.transform(processed_collection_r)\n",
    "y_resume = svm.predict(resume_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {v: k for k, v in seen.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5507: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1684: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = infer_fill_value(value)\n",
      "/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-9536e0226ec5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mjoins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjoins\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-9536e0226ec5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mjoins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjoins\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     21\u001b[0m         return [\n\u001b[1;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         ]\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         ]\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resume_list['pred_category'] = y_resume\n",
    "resume_list.pred_category = resume_list.pred_category.astype('object')\n",
    "idx_v = {}\n",
    "for idx, row in resume_list.iterrows():\n",
    "    resume_list.at[idx, 'pred_category_text'] = categories[row.pred_category]\n",
    "    post = row.text\n",
    "    id = row.postid\n",
    "    tokens = nltk.word_tokenize(post)\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "    tokens = [token for token in tokens if not token in stopwords.words('english') if token.isalpha()]\n",
    "    joins = \" \".join(tokens)\n",
    "    v = vectorizer.transform([joins])\n",
    "    if row.pred_category in idx_v:\n",
    "        idx_v[row.pred_category].append((v,id))\n",
    "    else:\n",
    "        idx_v[row.pred_category] = [(v,id)]\n",
    "        \n",
    "jobs_idx_v = {}\n",
    "for idx, row in job_list.iterrows():\n",
    "    post = row.text\n",
    "    id = row.postid\n",
    "    tokens = nltk.word_tokenize(post)\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "    tokens = [token for token in tokens if not token in stopwords.words('english') if token.isalpha()]\n",
    "    joins = \" \".join(tokens)\n",
    "    v = vectorizer.transform([joins])\n",
    "    if row.category in jobs_idx_v:\n",
    "        jobs_idx_v[row.category].append((v,id))\n",
    "    else:\n",
    "        jobs_idx_v[row.category] = [(v,id)]\n",
    "    \n",
    "resume_list.to_csv(\"predicted_categories.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "first_job = x_train_m[0]\n",
    "first_job_y = y_train[0]\n",
    "pred_resumes = pd.read_csv(\"predicted_categories.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "jobs_matching_resumes = {}\n",
    "\n",
    "for job_category in jobs_idx_v:\n",
    "    jobs = jobs_idx_v[job_category][:3]\n",
    "    for job_vtext, job_id in jobs:\n",
    "        v = job_vtext.toarray()\n",
    "        if job_category in idx_v:\n",
    "            v_text = idx_v[job_category]\n",
    "            top = None\n",
    "            sims = []\n",
    "            for text,id in v_text:\n",
    "                sim = cosine_similarity([v[0,:]],[text.toarray()[0,:]])\n",
    "#                 sim = jaccard_score(v[0,:],text.toarray()[0,:], average='weighted')\n",
    "#                 print(sim)\n",
    "                sims.append((sim,id))\n",
    "\n",
    "            sims.sort(key=lambda x: x[0], reverse=True)\n",
    "            jobs_matching_resumes[job_id] = sims[:3]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('jobs_top_resumes.csv', 'w')\n",
    "f.write('job_id,job_text,res1_id,res1_text,res2_id,res2_text,res3_id,res3_text\\n')\n",
    "for job_id in jobs_matching_resumes:\n",
    "    job_text = list(df[df['postid'] == job_id].text)[0].replace(',','')\n",
    "    f.write(\"{},{}\".format(job_id, job_text))\n",
    "    for resume in jobs_matching_resumes[job_id]:\n",
    "        if resume[0] > 0.04:\n",
    "            resume_id = resume[1]\n",
    "            resume_text = list(df[df['postid'] == resume_id].text)[0].replace(',','')\n",
    "            f.write(\",{},{}\".format(resume_id,resume_text))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes_matching_jobs = {}\n",
    "\n",
    "for category in idx_v:\n",
    "    potential_list = jobs_idx_v[category]\n",
    "    for resume_vtext, resume_id in idx_v[category]:\n",
    "        resume_vtext = resume_vtext.toarray()\n",
    "        \n",
    "        sims = []\n",
    "        for job_vtext, job_id in potential_list:\n",
    "            job_vtext = job_vtext.toarray()\n",
    "            \n",
    "            sim = cosine_similarity([resume_vtext[0,:]],[job_vtext[0,:]])\n",
    "            sims.append((sim[0][0],job_id))\n",
    "            \n",
    "        sims.sort(key=lambda x: x[0], reverse=True)\n",
    "        resumes_matching_jobs[resume_id] = sims[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('resume_top_jobs.csv', 'w')\n",
    "f.write('res_id,res_text,job1_id,job1_text,job2_id,job2_text,job3_id,job3_text\\n')\n",
    "for res_id in resumes_matching_jobs:\n",
    "    res_text = list(df[df['postid'] == res_id].text)[0].replace(',','')\n",
    "    f.write(\"{},{}\".format(res_id, res_text))\n",
    "    for job in resumes_matching_jobs[res_id]:\n",
    "        if job[0] > 0.04:\n",
    "            job_id = job[1]\n",
    "            job_text = list(df[df['postid'] == job_id].text)[0].replace(',','')\n",
    "            f.write(\",{},{}\".format(job_id,job_text))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
